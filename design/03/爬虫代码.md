```python
//1.微信公众号
import sys
 
reload(sys)
sys.setdefaultencoding('utf-8')
 
from urllib import quote
from pyquery import PyQuery as pq
 
import requests
import time
import re
import os
 
from selenium.webdriver import Chrome
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.support.wait import WebDriverWait
 
 
def get_search_result_by_keywords(sogou_search_url):
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0'}
 
    timeout = 5
    # 爬虫模拟在一个request.session中完成
    s = requests.Session()
    log(u'搜索地址为：%s' % sogou_search_url)
    return s.get(sogou_search_url, headers=headers, timeout=timeout).content
 
 
# 获得公众号主页地址
def get_wx_url_by_sougou_search_html(sougou_search_html):
    doc = pq(sougou_search_html)
    return doc('div[class=txt-box]')('p[class=tit]')('a').attr('href')
 
 
# 使用webdriver 加载公众号主页内容，主要是js渲染的部分
def get_selenium_js_html(url):
    options = Options()
    options.add_argument('-headless')  # 无头参数
    driver = Chrome(executable_path='chromedriver', chrome_options=options)
    wait = WebDriverWait(driver, timeout=10)
 
    driver.get(url)
    time.sleep(3)
    # 执行js得到整个页面内容
    html = driver.execute_script("return document.documentElement.outerHTML")
    driver.close()
    return html
 
 
# 获取公众号文章内容
def parse_wx_articles_by_html(selenium_html):
    doc = pq(selenium_html)
    return doc('div[class="weui_media_box appmsg"]')
 
 
# 将获取到的文章转换为字典
def switch_arctiles_to_list(articles):
    # 定义存贮变量
    articles_list = []
    i = 1
 
    # 遍历找到的文章，解析里面的内容
    if articles:
        for article in articles.items():
            log(u'开始整合(%d/%d)' % (i, len(articles)))
            # 处理单个文章
            articles_list.append(parse_one_article(article))
            i += 1
    return articles_list
 
 
# 解析单篇文章
def parse_one_article(article):
    article_dict = {}
 
    # 获取标题
    title = article('h4[class="weui_media_title"]').text().strip()
    ###log(u'标题是： %s' % title)
    # 获取标题对应的地址
    url = 'http://mp.weixin.qq.com' + article('h4[class="weui_media_title"]').attr('hrefs')
    log(u'地址为： %s' % url)
    # 获取概要内容
    summary = article('.weui_media_desc').text()
    log(u'文章简述： %s' % summary)
    # 获取文章发表时间
    date = article('.weui_media_extra_info').text().strip()
    log(u'发表时间为： %s' % date)
    # 获取封面图片
    pic = parse_cover_pic(article)
 
    # 返回字典数据
    return {
        'title': title,
        'url': url,
        'summary': summary,
        'date': date,
        'pic': pic
    }
 
 
# 查找封面图片，获取封面图片地址
def parse_cover_pic(article):
    pic = article('.weui_media_hd').attr('style')
 
    p = re.compile(r'background-image:url\((.*?)\)')
    rs = p.findall(pic)
    log(u'封面图片是：%s ' % rs[0] if len(rs) > 0 else '')
 
    return rs[0] if len(rs) > 0 else ''
 
 
# 自定义log函数，主要是加上时间
def log(msg):
    print u'%s: %s' % (time.strftime('%Y-%m-%d_%H-%M-%S'), msg)
 
 
# 验证函数
def need_verify(selenium_html):
    ' 有时候对方会封锁ip，这里做一下判断，检测html中是否包含id=verify_change的标签，有的话，代表被重定向了，提醒过一阵子重试 '
    return pq(selenium_html)('#verify_change').text() != ''
 
 
# 创建公众号命名的文件夹
def create_dir(keywords):
    if not os.path.exists(keywords):
        os.makedirs(keywords)
 
        # 爬虫主函数
 
 
def run(keywords):
    ' 爬虫入口函数 '
    # Step 0 ：  创建公众号命名的文件夹
    create_dir(keywords)
 
    # 搜狐微信搜索链接入口
    sogou_search_url = 'http://weixin.sogou.com/weixin?type=1&query=%s&ie=utf8&s_from=input&_sug_=n&_sug_type_=' % quote(
        keywords)
 
    # Step 1：GET请求到搜狗微信引擎，以微信公众号英文名称作为查询关键字
    log(u'开始获取，微信公众号英文名为：%s' % keywords)
    log(u'开始调用sougou搜索引擎')
    sougou_search_html = get_search_result_by_keywords(sogou_search_url)
 
    # Step 2：从搜索结果页中解析出公众号主页链接
    log(u'获取sougou_search_html成功，开始抓取公众号对应的主页wx_url')
    wx_url = get_wx_url_by_sougou_search_html(sougou_search_html)
    log(u'获取wx_url成功，%s' % wx_url)
 
    # Step 3：Selenium+PhantomJs获取js异步加载渲染后的html
    log(u'开始调用selenium渲染html')
    selenium_html = get_selenium_js_html(wx_url)
 
    # Step 4: 检测目标网站是否进行了封锁
    if need_verify(selenium_html):
        log(u'爬虫被目标网站封锁，请稍后再试')
    else:
        # Step 5: 使用PyQuery，从Step 3获取的html中解析出公众号文章列表的数据
        log(u'调用selenium渲染html完成，开始解析公众号文章')
        articles = parse_wx_articles_by_html(selenium_html)
        log(u'抓取到微信文章%d篇' % len(articles))
 
        # Step 6: 把微信文章数据封装成字典的list
        log(u'开始整合微信文章数据为字典')
        articles_list = switch_arctiles_to_list(articles)
        return [content['title'] for content in articles_list]
 
​```
 
main入口函数：
 
​```python
# coding: utf8
import spider_weixun_by_sogou
 
if __name__ == '__main__':
 
    gongzhonghao = raw_input(u'input weixin gongzhonghao:')
    if not gongzhonghao:
        gongzhonghao = 'spider'
    text = " ".join(spider_weixun_by_sogou.run(gongzhonghao))
 
    print text
    
    
    
    
 //2.爬取某个学校网站所有最近通知：
    
import urllib2//urllib2  下载网站包
from bs4 import BeautifulSoup//解析网站包
import json//将获取信息写入json格式
import sys //转换编码方式

#sys系统编码为ascii 通过下面两行代码更改默认编码
reload(sys)
sys.setdefaultencoding(‘utf-8’)
resultData = []//用来存放每次抓取的信息
//http://www.snnu.edu.cn/tzgg/217.htm
//urllib2 抓取网页
url = ‘http://www.snnu.edu.cn/tzgg/’			
count =1												
for i in range(1, 219):
num = 219 - i
url_com = url + str(num) + “.htm”//循环组成所有目标url
response = urllib2.urlopen(url_com)
print response
//charset = chardet.detect(url)
//print charset   //获取目标网页的编码方式
//print response.getcode() #检测是否抓取成功输出200
//cont = response.read().decode(‘utf-8’) 读出获得的网页
// BeautifulSoup 解析网页
soup = BeautifulSoup(response, ‘html.parser’)
link = soup.find_all(class_=“con_newslist”)[0]  #寻找目标特征
links = link.find_all(“li”)
for li in links:
data = {
“title” : li.find(“a”).text
}
print count
count = count + 1
resultData.append(data)//将每次循环获取的数据写入数组
with open(‘result.json’, ‘wb’) as f://将数组写入相同目录resylt.json中  wb表示可读取 可写入
f.write(json.dumps(resultData).decode(“unicode-escape”))、、写入的这行代码跟开头sys配套更改最后写入编码


//3.爬取微博的热搜标题和部分图片

import urllib
import urllib2
from bs4 import BeautifulSoup
header ={
“Cookie”: “SINAGLOBAL=9411374167025.424.1552041554396; UOR=,v.ifeng.com; login_sid_t=6bc26fe217fbe09d05225f5654146e49; cross_origin_proto=SSL; _s_tentry=-; Apache=2129606814883.5312.1557713131893; ULV=1557713131897:3:2:1:2129606814883.5312.1557713131893:1557536542243; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WWahfmB37nahga6ao9IhFid5JpX5o275NHD95QcSK24ShM4e0-0Ws4DqcjJi–Ri-zXi-iWCs-LxK-LB–LBoqLxKqL1-eL1h.LxK.L1K2LB-zt; SSOLoginState=1557713230; SUB=_2A25x3KECDeRhGeBL6lsV9yfPzj-IHXVSq5XKrDV8PUNbmtAKLWHakW9NRyhOGFlGQ_u-eepwRtim_EAJSV9wzzbY; SUHB=0Ela5I-B2gp8kP; ALF=1589249232; wvr=6; webim_unReadCount=%7B%22time%22%3A1557713248360%2C%22dm_pub_total%22%3A0%2C%22chat_group_pc%22%3A0%2C%22allcountNum%22%3A2%2C%22msgbox%22%3A0%7D”,
“User-Agent”: “Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36”
}
datas = []
pics = []
def loadpic(pic): //下载图片
for i in range(0,len(pic)):
pic1 = pic[i]
home = “C:\Users\snnucs\Desktop\pic\” + str(i) + “.jpg”
try:
urllib.urlretrieve(pic[i], home)
except Exception:
print “meiyoufuhegeshi”
def get_data(urls):
print len(urls)
for i in range(0,len(urls))?/用添加header的方式下载解析url
request = urllib2.Request(“https://s.weibo.com/”+ urls[i], headers=header)
response = urllib2.urlopen(request)
soup = BeautifulSoup(response, ‘html.parser’)
try:
pic = soup.find_all(class_=“wbs-feed”)[0].find_all(class_=“m-main”)[0].find(class_=“card-topic-a”).find(“img”)[“src”] //这里会出现问题 越界或者返回数据类型错误
except AttributeError:
print “meizhaodao”
except IndexError:
print “yuejie”
datas.append(pic)
return datas//返图偏
#     data = {
#         “titile” : soup.find_all(class_=“wbs-feed”)[0].find_all(class_=“m-main”)[0].find(class_=“card-topic-a”).find(class_=“info”).find(class_=“title”).find(“a”).text,
#         “pic” : soup.find_all(class_=“wbs-feed”)[0].find_all(class_=“m-main”)[0].find(class_=“card-topic-a”).find(“img”)[“src”]
#
#     }
#     datas.append(data)
# return datas
#     pics.append(pic)
# return pics  //注释的代码 爬取标题和照片但是有格式问题 没有完成待修改
def get_url(url_g)?/两次下载解析的目的是热搜51条里 要先下载一次获取 所有51条链接  之后再下载对应的
request = urllib2.Request(url_g,headers=header)//51链接的标题和图片
response = urllib2.urlopen(request)
soup = BeautifulSoup(response,‘html.parser’)
urls = []
url_f = soup.find_all(class_=“wbs-hotrank”)[0].find_all(class_=“m-main”)[0].find_all(class_=“m-wrap”)[0].find_all(class_=“data”)[0].find_all(“a”)
print len(url_f)
for i in range(0,len(url_f)):
urls.append(url_f[i][‘href’])
return urls
if name ==“main”:
url = “https://s.weibo.com/top/summary?Refer=top_hot&topnav=1&wvr=6”
loadpic(get_data(get_url(url)))
print “ok”




//4.京东商品信息爬取
from selenium import webdriver
import time 

# 创建浏览器对象
driver = webdriver.Chrome()
# 访问京东首页
driver.get('https://www.jd.com/')
# 找到搜索框按钮,接收终端输入,发送到搜索框
text = driver.find_element_by_class_name('text')
key = input("请输入要搜索的内容:")
text.send_keys(key)
# 点击 搜索按钮
button = driver.find_element_by_class_name('button')
button.click()
time.sleep(2)

while True:
    # 执行脚本,进度条拉到最底部
    driver.execute_script(
     'window.scrollTo(0,document.body.\
    scrollHeight)')
    time.sleep(3)
    # 提取数据,分析数据
    rList = driver.find_elements_by_xpath(
               '//div[@id="J_goodsList"]//li')
    # rList : ['商品1节点对象','商品2节点对象']
    for r in rList:
        contentList = r.text.split('\n')
        price = contentList[0]
        name = contentList[1]
        commit = contentList[2]
        market = contentList[3]
        
        d = {
                "价格":price,
                "名称":name,
                "评论":commit,
                "商家":market,
                }
        with open("jd.json","a",encoding="utf-8") as f:
            f.write(str(d) + '\n')
        
    # 点击下一页,-1表示没找到
    if driver.page_source.find(
            'pn-next disabled') == -1:
        driver.find_element_by_class_name\
                   ('pn-next').click()
        time.sleep(3)
    else:
        print("爬取结束")
        break
#下一页能点 ：  pn-next
#下一页不能点： pn-next disabled
# 关闭浏览器
driver.quit()

```

